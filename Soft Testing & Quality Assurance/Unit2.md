  ### Q1. Explain types of test Artifacts.

  Test artifacts are essential documents and deliverables produced during the software testing process. They help in planning, designing, executing, and managing testing activities. Here are the main types of test artifacts:

1. **Test Plan**:
   - A document that outlines the testing strategy, objectives, schedule, resources, scope, and activities for a specific testing effort. It serves as a blueprint for the testing process.

2. **Test Cases**:
   - Detailed step-by-step instructions for testing specific functionality or features of the software. Each test case includes inputs, execution conditions, and expected results.

3. **Test Scripts**:
   - Automated scripts written for automated testing tools that perform predefined actions on the software to verify its behavior. They are used in regression testing, load testing, and other types of automated testing.

4. **Test Scenarios**:
   - High-level descriptions of what needs to be tested, typically representing user workflows or system operations. Test scenarios are used to ensure coverage of all possible use cases.

5. **Test Data**:
   - Data created or selected to be used during the execution of test cases. Test data includes input data, environment variables, and other data that influence the test outcome.

6. **Defect Reports (Bug Reports)**:
   - Documents that describe any flaws or issues found in the software. A defect report typically includes details such as defect ID, description, severity, steps to reproduce, and screenshots.

7. **Traceability Matrix**:
   - A document that maps and traces the relationship between test cases and requirements. It ensures that all requirements are covered by test cases and helps track changes in requirements.

8. **Test Summary Report**:
   - A document that summarizes the results of the testing activities. It includes information such as the number of test cases executed, defects found, defects resolved, and overall quality assessment of the software.

These artifacts are critical for managing the testing process, ensuring quality, and providing transparency to stakeholders.

---

### Q2. Differentiate between test plan & test strategy


## Test Plan vs. Test Strategy

| Feature | Test Plan | Test Strategy |
|---|---|---|
| Purpose | Provides a detailed blueprint for conducting testing activities. | Defines the overall approach and objectives for testing. |
| Scope | Defines the specific testing activities to be conducted. | Outlines the testing scope, objectives, and deliverables. |
| Level of Detail | Highly detailed, including test cases, test data, and resource allocation. | High-level overview of testing approach, methodologies, and principles. |
| Audience | Primarily for project team members, testers, and stakeholders involved in testing. | Primarily for project managers, stakeholders, and senior management. |
| Creation Time | Typically created after the test strategy is finalized. | Created early in the project lifecycle. |
| Flexibility | Can be modified during the testing process as needed. | Less flexible, as it establishes the foundation for the testing effort. |
| Focus | On executing specific test activities. | On defining the overall testing approach and objectives. |
| Relationship | The test plan is a subset of the test strategy. | The test strategy provides the framework for the test plan. |


---

#### Q3. Discuss Integration testing and Acceptance testing.

Here are 8 key points explaining the **entry and exit criteria of testing**:

### **Entry Criteria**:
1. **Requirements Document Availability**: All functional and non-functional requirements are clearly defined and documented.
2. **Test Plan and Test Cases Prepared**: A comprehensive test plan and well-documented test cases are ready for execution.
3. **Test Environment Setup**: The test environment is fully configured and matches the production or expected environment.
4. **Code Development Completion**: The code or software build for the module or system is completed and unit-tested by developers.
5. **Tools and Resources in Place**: All testing tools, test data, and necessary resources are available for test execution.
6. **Acceptance of Entry Review**: A review meeting is conducted to confirm readiness for testing based on the above criteria.
7. **Bug Tracking System**: A bug-tracking system is established, and the process for logging defects is defined.
8. **Stakeholder Approval**: Formal approval from stakeholders to begin the testing phase is obtained.

### **Exit Criteria**:
1. **Test Case Execution Complete**: All planned test cases have been executed, and the test coverage meets the expected goals.
2. **Defect Resolution**: All critical and high-priority defects have been identified, fixed, and retested.
3. **No Major Bugs**: There are no unresolved major bugs that block the functionality, and the remaining minor issues are documented.
4. **Test Summary Report**: A detailed test summary report is prepared, documenting test results, defect status, and coverage.
5. **Quality Benchmarks Met**: The system meets the defined quality standards or benchmarks, such as performance, security, or usability criteria.
6. **User Acceptance**: Acceptance testing is completed, and the system is approved by the stakeholders or end-users.
7. **Regression Testing Complete**: All necessary regression tests are completed to ensure that fixes haven't affected other areas of the application.
8. **Sign-Off from Stakeholders**: Formal sign-off is obtained from stakeholders, indicating that the testing phase is complete, and the product is ready for deployment.

These entry and exit criteria ensure a structured and efficient testing process, helping manage expectations and deliver a quality product.

### Q4. Define and Explain configuration management.

## Configuration Management

**Definition:** Configuration management is a systematic approach to controlling changes to a system's configuration throughout its lifecycle. It ensures that the system's components and their relationships remain consistent and traceable.

**Key Points:**

1. **Identification:** Identifying and documenting all components of the system, including hardware, software, firmware, and data.
2. **Control:** Establishing procedures for controlling changes to the system's configuration, such as reviews, approvals, and authorizations.
3. **Status Accounting:** Maintaining accurate records of the system's configuration, including version numbers, release dates, and change history.
4. **Auditing:** Conducting regular audits to verify the accuracy and completeness of the configuration management process.
5. **Reporting:** Providing timely reports on the system's configuration status and any changes that have occurred.
6. Build and Release Management: Ensuring that the correct versions of software and components are deployed in production. It minimizes risks and errors during deployment.

---

 ### Q5. Differentiate between verification and validation.
